{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 2.7.10 |Anaconda 2.3.0 (64-bit)| (default, Nov  7 2015, 13:18:40) [MSC v.1500 64 bit (AMD64)]\n",
      "Pymongo version 3.1.1\n",
      "\n",
      "Connecting ...\n",
      "\n",
      "Getting database ...\n",
      "\n",
      "Authenticating ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a database already created on mongolab \n",
    "server = 'ds059694.mongolab.com'\n",
    "port = 59694\n",
    "db_name = 'deepreader'\n",
    "username = 'deepreaderuser'\n",
    "password = '1ecolequimonte45'\n",
    "\n",
    "from pymongo import MongoClient as Connection\n",
    "\n",
    "# from pymongo import Connection\n",
    "\n",
    "# what versions are we using\n",
    "import sys\n",
    "print 'Python version', sys.version\n",
    "\n",
    "import pymongo\n",
    "print 'Pymongo version', pymongo.version\n",
    "##\n",
    "\n",
    "# connect to server\n",
    "print '\\nConnecting ...'\n",
    "conn = Connection(server, port)\n",
    "\n",
    "# Get the database\n",
    "print '\\nGetting database ...'\n",
    "db = conn[db_name]\n",
    "\n",
    "# Have to authenticate to get access\n",
    "print '\\nAuthenticating ...'\n",
    "db.authenticate(username, password)\n",
    "# from pymongo import MongoClient\n",
    "# client = MongoClient()\n",
    "# db = client.izidb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------> english word frequencies loaded\n",
      "--------------------> english word frequencies loaded\n"
     ]
    }
   ],
   "source": [
    "from scripts import izi\n",
    "reload(izi)\n",
    "\n",
    "n_topics = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write LDA topics to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# izi.saveTopicsAsTxt(u'../masc/0LDA_TOPIC/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get files list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n",
      "1412 Standard English Review-en-ru-T.mxliff\n"
     ]
    }
   ],
   "source": [
    "root = u\"../izi_data/\"\n",
    "filelist = izi.getFileList(root)\n",
    "print len(filelist)\n",
    "example =  filelist[0]\n",
    "print example[len(root):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_str(value):\n",
    "    if type(value) == str:\n",
    "    # Ignore errors even if the string is not proper UTF-8 or has\n",
    "    # broken marker bytes.\n",
    "    # Python built-in function unicode() can do this.\n",
    "        value = unicode(value, \"utf-8\", errors=\"ignore\")\n",
    "    else:\n",
    "    # Assume the value object has proper __unicode__() method\n",
    "        value = unicode(value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def loadTranslator(path):   \n",
    "    if path[-4:] == 'liff':\n",
    "        soup = BeautifulSoup( open(path), 'lxml')\n",
    "        for string in soup.find_all(\"file\"):\n",
    "            try:\n",
    "                return string['m:task-id']\n",
    "            except:\n",
    "                return -1\n",
    "            \n",
    "            \n",
    "def insertDoc(path, root = root, prefix = ''):\n",
    "    document = dict()\n",
    "    document['title'] = prefix + '___' + path[len(root):].replace( ' ', '_')\n",
    "#     document['title'] = path[len(root):].replace( ' ', '_')\n",
    "    document['type'] = prefix\n",
    "    document['translator'] = loadTranslator(path)\n",
    "    current_translator = db.translators.find_one({'name': document['translator']})\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        full_text = izi.loadText(path)\n",
    "    #     full_text = unicode(full_text, errors='ignore')\n",
    "        full_text = safe_str(full_text)\n",
    "\n",
    "        if len(full_text) > 500:\n",
    "            if not current_translator:\n",
    "                translator = dict()\n",
    "                translator['name'] = document['translator']\n",
    "                translator['document'] = []\n",
    "                id_current_translator = db.translators.save(translator)\n",
    "                current_translator = db.translators.find_one({'name': document['translator']})\n",
    "\n",
    "            # text and tokens\n",
    "            tokens = izi.tokenize( full_text)\n",
    "            document['full_text'] = full_text\n",
    "            document['tokens'] = tokens\n",
    "            # topics\n",
    "            topics =  izi.topicsFromTokens(izi.tokenize(full_text))\n",
    "            semantic_vec = [0.] * n_topics\n",
    "            for i in topics:\n",
    "                semantic_vec[i[0]] = i[1]\n",
    "            document['semantic_vec'] = semantic_vec\n",
    "            # complexity\n",
    "            document['complexity'] = izi.complexityAlongtheText(full_text)\n",
    "            # topic distribution\n",
    "            document['full_topics'] = izi.getTopicDistributionData( document['full_text'], document['semantic_vec'])\n",
    "            # significants words\n",
    "            document['significantWords'] = izi.getMostSignificantWordsData(document['tokens'] , document['semantic_vec'])\n",
    "            # significant words graph\n",
    "            document['topicsGraph'] = izi.SignificantWordsGraph(document['tokens'] , document['semantic_vec'] )\n",
    "\n",
    "            ##\n",
    "            # savec doc\n",
    "            current_id = db.documents.save(document)\n",
    "            current_translator['document'].append(current_id)\n",
    "            db.translators.update({'name': document['translator']}, {'name': document['translator'],\\\n",
    "                                                                     'document': current_translator['document']})\n",
    "\n",
    "\n",
    "            ###################\n",
    "            # create links\n",
    "            cursor = db.documents.find()\n",
    "            for doc in cursor:\n",
    "                y = doc['semantic_vec']\n",
    "                y_id = doc[\"_id\"]\n",
    "                if y_id != current_id:\n",
    "                    s = izi.getSimilarity( semantic_vec, y)\n",
    "                    db.similarities.insert({'source': current_id , 'target': y_id, 'value': s})\n",
    "    except:\n",
    "        print \"############################################################\"\n",
    "        print \"#### ERROR: \" + document['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class idGenerator:\n",
    "\tdef __init__(self):\n",
    "\t\tself.id = 0\n",
    "\tdef get(self):\n",
    "\t\tself.id += 1\n",
    "\t\treturn self.id - 1\n",
    "    \n",
    "def getLastAdded():\n",
    "\tcurrent_id = None\n",
    "\tfor d in db.documents.find().sort(\"_id\", -1).limit(1) :\n",
    "\t\tcurrent_id = d\n",
    "\treturn db.documents.find_one( { \"_id\" : current_id[\"_id\"] })\n",
    "\n",
    "\n",
    "def getGraph():\n",
    "    SIMILARITY_CUTOFF = 0.85\n",
    "    removed = set()\n",
    "    lasDoc = getLastAdded()\n",
    "    semantic_vectors = dict()\n",
    "    list_ids = db.documents.find().distinct(\"_id\")\n",
    "    current_id = lasDoc[\"_id\"]\n",
    "\n",
    "    id2db = dict()\n",
    "    gen = idGenerator()\n",
    "\n",
    "    nodes = []\n",
    "    k = 0\n",
    "    print \"%s nodes\" %len(list_ids)\n",
    "    for ii in list_ids:\n",
    "        print k,\n",
    "        k+=1\n",
    "        doc = db.documents.find_one( {'_id' : ii} )\n",
    "        node = dict()\n",
    "        tmp_id = doc[\"_id\"]\n",
    "        i = gen.get()\n",
    "        id2db[str(tmp_id)] = i\n",
    "        node[\"id\"] = i\n",
    "        if doc['type'] == '0LDA_TOPIC':\n",
    "            node['color'] = \"#cccccc\"\n",
    "            node['size'] = 7\n",
    "        else:\n",
    "            node['color'] = \"#555555\"\n",
    "            node['size'] = 5\n",
    "        node['id_db'] = str(tmp_id)\n",
    "        node['name'] = doc['title']\n",
    "#         # remove unconnected topics\n",
    "#         cursor = db.similarities.find()\n",
    "#         edges = []\n",
    "#         e_count = 0\n",
    "#         if doc['type'] != '0LDA_TOPIC':\n",
    "#             e_count = 3\n",
    "#         else:\n",
    "#             for e in cursor:\n",
    "#                 if (e['value'] > SIMILARITY_CUTOFF) and ( str(e['source']) != str(e['target']) ):\n",
    "#                     if ( str(e['source']) == str(tmp_id) ) or ( str(e['target']) == str(tmp_id) ):\n",
    "#                         print (str(tmp_id) + ' ' + str( e_count)),\n",
    "#                         removed.add( str(e['source']) )\n",
    "#                         removed.add( str(e['target']) )\n",
    "#                         e_count += 1\n",
    "            \n",
    "#         if e_count > 0:\n",
    "#             nodes.append(node)\n",
    "        nodes.append(node)\n",
    "\n",
    "    cursor = db.similarities.find()\n",
    "    edges = []\n",
    "    for e in cursor:\n",
    "        if e['value'] > SIMILARITY_CUTOFF:\n",
    "            a = e['source']\n",
    "            b = e['target']\n",
    "            if (str(a) not in removed) and (str(b) not in removed):\n",
    "                edge = {'source': id2db[str(a)] , 'target': id2db[str(b)], 'value': e['value']}\n",
    "                edges.append(edge)\n",
    "\n",
    "\n",
    "    graph = dict()\n",
    "    graph['nodes'] = nodes\n",
    "    graph['links'] = edges\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root2 = u\"../brown/\"\n",
    "import nltk\n",
    "brown_ids = nltk.corpus.brown.fileids()\n",
    "for i in brown_ids:\n",
    "    f = open( root2 + i + \".txt\", \"w\")\n",
    "    f.write(nltk.corpus.brown.raw(i) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in izi.getFileList(root2):\n",
    "    insertDoc(f, root = root2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cursor = db.documents.find_one({ \"_id\": id})\n",
    "# for document in cursor:\n",
    "#     print document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "root_scripts = u\"../scripts_movies/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('raw_script_urls.txt', 'r')\n",
    "for l in f.readlines():\n",
    "    ll =  [ u.rstrip() for u in l.split(' +++$+++ ') ]\n",
    "    title = ll[1]\n",
    "    title = title.replace(\" \", \"_\")\n",
    "    link = ll[2]\n",
    "    try:\n",
    "        p = urllib.urlopen(link).read()\n",
    "        if len(p) > 1000:\n",
    "            print link + ' ' + title\n",
    "            path = root_scripts + title + \".txt\"\n",
    "            file = open( path, \"w\")\n",
    "            \n",
    "            file.write( p )\n",
    "            insertDoc(path, root = root_scripts)\n",
    "        else:\n",
    "            print 'TOO SHORT: ' + link + ' ' + title\n",
    "    except:\n",
    "        print \"ERROR: \" + title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print izi.getFileList(root_scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fails = []\n",
    "print len(izi.getFileList(root_scripts))\n",
    "i = 0\n",
    "for f in izi.getFileList(root_scripts):\n",
    "    try:\n",
    "        insertDoc(f, root = root_scripts)\n",
    "        print i,\n",
    "    except:\n",
    "        fails.append(f)\n",
    "        print \"#\",\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# masc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# root_masc = u\"../masc/\"\n",
    "# k = 0\n",
    "# limit = 4\n",
    "# for f in izi.getFileList(root_masc):\n",
    "#     sub_root = f+'/'\n",
    "#     for ff in izi.getFileList(sub_root):\n",
    "#         print str(k) + '  |   ' +  str(ff)\n",
    "#         if True: #k < limit:\n",
    "#             insertDoc(ff, root = sub_root, prefix = f[len(root_masc):])\n",
    "#             k+=1\n",
    "#         else:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222 nodes\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:2: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectId('566587469f5c8d2494e4f78c')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "db.graph.drop()\n",
    "db.graph.insert(  getGraph() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scripts import izi\n",
    "reload(izi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fails = []\n",
    "i = 0\n",
    "maxx = 5\n",
    "for f in filelist:\n",
    "    if True: #i < maxx:\n",
    "        try:\n",
    "            print f\n",
    "            insertDoc(f)\n",
    "        except:\n",
    "            fails.append(f)\n",
    "            print \"#\"\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "print\n",
    "print\n",
    "print \"%s fails\" %len(fails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in fails:\n",
    "    print f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor = db.documents.find_one()\n",
    "for document in cursor:\n",
    "    print document\n",
    "    \n",
    "    \n",
    "x = cursor[\"_id\"]\n",
    "\n",
    "db.documents.find_one({ \"_id\": x})[\"full_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.similarities.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.translators.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reset collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.documents.drop()\n",
    "db.similarities.drop()\n",
    "db.graph.drop()\n",
    "db.translators.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
