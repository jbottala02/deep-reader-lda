{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 2.7.10 |Anaconda 2.3.0 (64-bit)| (default, Nov  7 2015, 13:18:40) [MSC v.1500 64 bit (AMD64)]\n",
      "Pymongo version 3.1.1\n",
      "\n",
      "Connecting ...\n",
      "\n",
      "Getting database ...\n",
      "\n",
      "Authenticating ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a database already created on mongolab \n",
    "server = 'ds059694.mongolab.com'\n",
    "port = 59694\n",
    "db_name = 'deepreader'\n",
    "username = 'deepreaderuser'\n",
    "password = '1ecolequimonte45'\n",
    "\n",
    "from pymongo import MongoClient as Connection\n",
    "\n",
    "# from pymongo import Connection\n",
    "\n",
    "# what versions are we using\n",
    "import sys\n",
    "print 'Python version', sys.version\n",
    "\n",
    "import pymongo\n",
    "print 'Pymongo version', pymongo.version\n",
    "##\n",
    "\n",
    "# connect to server\n",
    "print '\\nConnecting ...'\n",
    "conn = Connection(server, port)\n",
    "\n",
    "# Get the database\n",
    "print '\\nGetting database ...'\n",
    "db = conn[db_name]\n",
    "\n",
    "# Have to authenticate to get access\n",
    "print '\\nAuthenticating ...'\n",
    "db.authenticate(username, password)\n",
    "# from pymongo import MongoClient\n",
    "# client = MongoClient()\n",
    "# db = client.izidb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------> english word frequencies loaded\n",
      "--------------------> english word frequencies loaded\n"
     ]
    }
   ],
   "source": [
    "from scripts import izi\n",
    "reload(izi)\n",
    "\n",
    "n_topics = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write LDA topics to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# izi.saveTopicsAsTxt(u'../masc/0LDA_TOPIC/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get files list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n",
      "1412 Standard English Review-en-ru-T.mxliff\n"
     ]
    }
   ],
   "source": [
    "root = u\"../izi_data/\"\n",
    "filelist = izi.getFileList(root)\n",
    "print len(filelist)\n",
    "example =  filelist[0]\n",
    "print example[len(root):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_str(value):\n",
    "    if type(value) == str:\n",
    "    # Ignore errors even if the string is not proper UTF-8 or has\n",
    "    # broken marker bytes.\n",
    "    # Python built-in function unicode() can do this.\n",
    "        value = unicode(value, \"utf-8\", errors=\"ignore\")\n",
    "    else:\n",
    "    # Assume the value object has proper __unicode__() method\n",
    "        value = unicode(value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def loadTranslator(path):   \n",
    "    if path[-4:] == 'liff':\n",
    "        soup = BeautifulSoup( open(path), 'lxml')\n",
    "        for string in soup.find_all(\"file\"):\n",
    "            try:\n",
    "                return string['m:task-id']\n",
    "            except:\n",
    "                return -1\n",
    "            \n",
    "            \n",
    "def insertDoc(path, root = root, prefix = ''):\n",
    "    document = dict()\n",
    "    document['title'] = prefix + '___' + path[len(root):].replace( ' ', '_')\n",
    "#     document['title'] = path[len(root):].replace( ' ', '_')\n",
    "    document['type'] = prefix\n",
    "    document['translator'] = loadTranslator(path)\n",
    "    current_translator = db.translators.find_one({'name': document['translator']})\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        full_text = izi.loadText(path)\n",
    "    #     full_text = unicode(full_text, errors='ignore')\n",
    "        full_text = safe_str(full_text)\n",
    "\n",
    "        if len(full_text) > 500:\n",
    "            if not current_translator:\n",
    "                translator = dict()\n",
    "                translator['name'] = document['translator']\n",
    "                translator['document'] = []\n",
    "                id_current_translator = db.translators.save(translator)\n",
    "                current_translator = db.translators.find_one({'name': document['translator']})\n",
    "\n",
    "            # text and tokens\n",
    "            tokens = izi.tokenize( full_text)\n",
    "            document['full_text'] = full_text\n",
    "            document['tokens'] = tokens\n",
    "            # topics\n",
    "            topics =  izi.topicsFromTokens(izi.tokenize(full_text))\n",
    "            semantic_vec = [0.] * n_topics\n",
    "            for i in topics:\n",
    "                semantic_vec[i[0]] = i[1]\n",
    "            document['semantic_vec'] = semantic_vec\n",
    "            # complexity\n",
    "            document['complexity'] = izi.complexityAlongtheText(full_text)\n",
    "            # topic distribution\n",
    "            document['full_topics'] = izi.getTopicDistributionData( document['full_text'], document['semantic_vec'])\n",
    "            # significants words\n",
    "            document['significantWords'] = izi.getMostSignificantWordsData(document['tokens'] , document['semantic_vec'])\n",
    "            # significant words graph\n",
    "            document['topicsGraph'] = izi.SignificantWordsGraph(document['tokens'] , document['semantic_vec'] )\n",
    "\n",
    "            ##\n",
    "            # savec doc\n",
    "            current_id = db.documents.save(document)\n",
    "            current_translator['document'].append(current_id)\n",
    "            db.translators.update({'name': document['translator']}, {'name': document['translator'],\\\n",
    "                                                                     'document': current_translator['document']})\n",
    "\n",
    "\n",
    "            ###################\n",
    "            # create links\n",
    "            cursor = db.documents.find()\n",
    "            for doc in cursor:\n",
    "                y = doc['semantic_vec']\n",
    "                y_id = doc[\"_id\"]\n",
    "                if y_id != current_id:\n",
    "                    s = izi.getSimilarity( semantic_vec, y)\n",
    "                    db.similarities.insert({'source': current_id , 'target': y_id, 'value': s})\n",
    "    except:\n",
    "        print \"############################################################\"\n",
    "        print \"#### ERROR: \" + document['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class idGenerator:\n",
    "\tdef __init__(self):\n",
    "\t\tself.id = 0\n",
    "\tdef get(self):\n",
    "\t\tself.id += 1\n",
    "\t\treturn self.id - 1\n",
    "    \n",
    "def getLastAdded():\n",
    "\tcurrent_id = None\n",
    "\tfor d in db.documents.find().sort(\"_id\", -1).limit(1) :\n",
    "\t\tcurrent_id = d\n",
    "\treturn db.documents.find_one( { \"_id\" : current_id[\"_id\"] })\n",
    "\n",
    "\n",
    "def getGraph():\n",
    "    SIMILARITY_CUTOFF = 0.85\n",
    "\n",
    "    lasDoc = getLastAdded()\n",
    "    semantic_vectors = dict()\n",
    "    list_ids = db.documents.find().distinct(\"_id\")\n",
    "    current_id = lasDoc[\"_id\"]\n",
    "\n",
    "    id2db = dict()\n",
    "    gen = idGenerator()\n",
    "\n",
    "    nodes = []\n",
    "    for ii in list_ids:\n",
    "        doc = db.documents.find_one( {'_id' : ii} )\n",
    "        node = dict()\n",
    "        tmp_id = doc[\"_id\"]\n",
    "        i = gen.get()\n",
    "        id2db[str(tmp_id)] = i\n",
    "        node[\"id\"] = i\n",
    "        if doc['type'] == '0LDA_TOPIC':\n",
    "            node['color'] = \"#cccccc\"\n",
    "            node['size'] = 7\n",
    "        else:\n",
    "            node['color'] = \"#555555\"\n",
    "            node['size'] = 5\n",
    "        node['id_db'] = str(tmp_id)\n",
    "        node['name'] = doc['title']\n",
    "        # remove unconnected topics\n",
    "        cursor = db.similarities.find()\n",
    "        edges = []\n",
    "        e_count = 0\n",
    "        if doc['type'] != '0LDA_TOPIC':\n",
    "            e_count = 3\n",
    "        for e in cursor:\n",
    "            if ( str(e['source']) == str(tmp_id) ) or ( str(e['target']) == str(tmp_id) ):\n",
    "                e_count += 1\n",
    "            \n",
    "        if e_count > 0:\n",
    "            nodes.append(node)\n",
    "\n",
    "    cursor = db.similarities.find()\n",
    "    edges = []\n",
    "    for e in cursor:\n",
    "        if e['value'] > SIMILARITY_CUTOFF:\n",
    "            a = e['source']\n",
    "            b = e['target']\n",
    "            edge = {'source': id2db[str(a)] , 'target': id2db[str(b)], 'value': e['value']}\n",
    "            edges.append(edge)\n",
    "\n",
    "\n",
    "    graph = dict()\n",
    "    graph['nodes'] = nodes\n",
    "    graph['links'] = edges\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root2 = u\"../brown/\"\n",
    "import nltk\n",
    "brown_ids = nltk.corpus.brown.fileids()\n",
    "for i in brown_ids:\n",
    "    f = open( root2 + i + \".txt\", \"w\")\n",
    "    f.write(nltk.corpus.brown.raw(i) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in izi.getFileList(root2):\n",
    "    insertDoc(f, root = root2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cursor = db.documents.find_one({ \"_id\": id})\n",
    "# for document in cursor:\n",
    "#     print document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "root_scripts = u\"../scripts_movies/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('raw_script_urls.txt', 'r')\n",
    "for l in f.readlines():\n",
    "    ll =  [ u.rstrip() for u in l.split(' +++$+++ ') ]\n",
    "    title = ll[1]\n",
    "    title = title.replace(\" \", \"_\")\n",
    "    link = ll[2]\n",
    "    try:\n",
    "        p = urllib.urlopen(link).read()\n",
    "        if len(p) > 1000:\n",
    "            print link + ' ' + title\n",
    "            path = root_scripts + title + \".txt\"\n",
    "            file = open( path, \"w\")\n",
    "            \n",
    "            file.write( p )\n",
    "            insertDoc(path, root = root_scripts)\n",
    "        else:\n",
    "            print 'TOO SHORT: ' + link + ' ' + title\n",
    "    except:\n",
    "        print \"ERROR: \" + title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print izi.getFileList(root_scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fails = []\n",
    "print len(izi.getFileList(root_scripts))\n",
    "i = 0\n",
    "for f in izi.getFileList(root_scripts):\n",
    "    try:\n",
    "        insertDoc(f, root = root_scripts)\n",
    "        print i,\n",
    "    except:\n",
    "        fails.append(f)\n",
    "        print \"#\",\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# masc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# root_masc = u\"../masc/\"\n",
    "# k = 0\n",
    "# limit = 4\n",
    "# for f in izi.getFileList(root_masc):\n",
    "#     sub_root = f+'/'\n",
    "#     for ff in izi.getFileList(sub_root):\n",
    "#         print str(k) + '  |   ' +  str(ff)\n",
    "#         if True: #k < limit:\n",
    "#             insertDoc(ff, root = sub_root, prefix = f[len(root_masc):])\n",
    "#             k+=1\n",
    "#         else:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:2: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectId('5664a6949f5c8d3440dcb980')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.graph.drop()\n",
    "db.graph.insert(  getGraph() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scripts import izi\n",
    "reload(izi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fails = []\n",
    "i = 0\n",
    "maxx = 5\n",
    "for f in filelist:\n",
    "    if True: #i < maxx:\n",
    "        try:\n",
    "            print f\n",
    "            insertDoc(f)\n",
    "        except:\n",
    "            fails.append(f)\n",
    "            print \"#\"\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "print\n",
    "print\n",
    "print \"%s fails\" %len(fails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in fails:\n",
    "    print f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor = db.documents.find_one()\n",
    "for document in cursor:\n",
    "    print document\n",
    "    \n",
    "    \n",
    "x = cursor[\"_id\"]\n",
    "\n",
    "db.documents.find_one({ \"_id\": x})[\"full_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.similarities.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.translators.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reset collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.documents.drop()\n",
    "db.similarities.drop()\n",
    "db.graph.drop()\n",
    "db.translators.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
